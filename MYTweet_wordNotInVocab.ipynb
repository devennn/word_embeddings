{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MYTweet_wordNotInVocab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPt9ZnatJNJFxHPS7oBPfvS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/devennn/word_embeddings/blob/master/MYTweet_wordNotInVocab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osLrCqmmwGGd",
        "colab_type": "code",
        "outputId": "6a63cb8a-8b35-4f83-ae39-aa1e184607ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8DHKCQJwl9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import os\n",
        "import tqdm\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAMCFt2Axe7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/datasets/tweets/'\n",
        "model_path = '/content/drive/My Drive/pretrained_model'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84ehocUZyBVu",
        "colab_type": "code",
        "outputId": "adbd2d2f-d0e8-4ac4-add1-a10d931b3fba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "with open(os.path.join(path, 'cleaned_tweets.txt'), 'r') as f:\n",
        "  covid = [s.strip() for s in f.readlines()]\n",
        "\n",
        "model = Word2Vec.load(os.path.join(model_path, 'twtV2_w2v.model'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKfYF7YOyOjX",
        "colab_type": "code",
        "outputId": "a8574998-d772-4bb7-e707-5f8797710993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "covid[:10]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['when will this be over',\n",
              " 'i miss those days when i sneeze people would politely say bless you now people just say get the fuck out of here',\n",
              " 'bond movie postponed cuz nobody wants to die',\n",
              " 'all these days software was scanned for virus now software engineers are scanned for virus',\n",
              " 'maklumat terkini mengenai sehingga jam pm petang tadi stay safe guys jangan lupa untuk utamakan kesihatan ok info',\n",
              " 'malaysia s covid update march total cases recovered currently under treatment out of cases cases are the st generation contact from case while cases are the nd generation contact of case',\n",
              " 'please come forward if you re one of these contacts stop',\n",
              " 'susulan covid kerajaan arab saudi telah mengambil langkah pencegahan ya allah ya allah yang maha melindungi lindungi kami daripada wabak penyakit ini',\n",
              " 'did not reveal the until a deputy minister got it after that it allowed cotizens who traveled to the infected region to go back but did not stamp their passports some were tested positive in saudi this is bad',\n",
              " 'they are among the recorded fatalities due to the in']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5_cq7jJzSMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_words = []\n",
        "for s in covid:\n",
        "  for w in s.split():\n",
        "    if w not in all_words:\n",
        "      all_words.append(w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3zXt44s0G15",
        "colab_type": "code",
        "outputId": "bdebffa8-2743-4305-d0bb-dd5a345a271e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(all_words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16499"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKJ074lhyQIL",
        "colab_type": "code",
        "outputId": "51b91ee4-94b8-4f71-e410-27d02a988881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "not_in_vocab = {}\n",
        "for w in tqdm.tqdm(all_words):\n",
        "  try:\n",
        "    model.wv.most_similar(w)\n",
        "  except KeyError:\n",
        "    if w not in not_in_vocab:\n",
        "      not_in_vocab[w] = 1\n",
        "    else:\n",
        "      not_in_vocab[w] += 1"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 16499/16499 [06:07<00:00, 44.83it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvT_mNVE2VKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Arrange\n",
        "not_in_vocab = {k: v for k, v in sorted(not_in_vocab.items(), key=lambda item: item[1], reverse=True)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2gf0crv4QJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "not_in_vocab_list = [w for w in not_in_vocab] # Convert to list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFIZcDdXqAjg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9d24fe1-c7c6-439c-a780-1825cf37e3af"
      },
      "source": [
        "len(not_in_vocab_list)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1280"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FE2Z5WRQVWH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find sentence with specified word\n",
        "# Only get the first sentence\n",
        "def get_sentence(w):\n",
        "  for s in covid:\n",
        "    if w in s.split(): return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPNzshMk3O6z",
        "colab_type": "text"
      },
      "source": [
        "#Find Closest string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiHup3MC2yBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import difflib\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHWn-vfw4uF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_closest_string(word):\n",
        "  result = difflib.get_close_matches(word, model_vocab, n=7)\n",
        "  print(\"{} -> {}\".format(word, result))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkUFPu-GYBrJ",
        "colab_type": "code",
        "outputId": "37c9d4ec-6251-4c56-fa1f-b2121544271c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model_vocab = list(model.wv.vocab)\n",
        "len(model_vocab)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "187985"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goCfjYWqCjiR",
        "colab_type": "text"
      },
      "source": [
        "Since the corpus is from Twitter, the vocab is huge. That's what makes it interesting because even one missing letter is counted as new word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0vxDtz93VrL",
        "colab_type": "code",
        "outputId": "9fe33629-d65a-4404-8f7d-9072031f8900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        }
      },
      "source": [
        "# For viewing\n",
        "# Randomly chose unknown word to test\n",
        "for _ in range(30): \n",
        "  find_closest_string(random.choice(not_in_vocab_list))"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mendaftarkan -> ['pendaftaran', 'mendaftar', 'daftarkan', 'mengantarkan', 'mendatangkan', 'pendaftran', 'menyatakan']\n",
            "faceshields -> ['faceshield', 'shields', 'freshies', 'shield', 'fields', 'faceid', 'childs']\n",
            "gazzetted -> ['gazetted', 'gazette', 'ezzette', 'abetted', 'gated', 'zetter', 'tatted']\n",
            "surachet -> ['sachet', 'rachet', 'surface', 'sachets', 'ratchet', 'superyacht', 'surat']\n",
            "tnl -> ['tngl', 'tn', 'tl', 'nl', 'tnggl', 'tngal', 'tkknl']\n",
            "kesihatanpun -> ['kesihatan', 'kesihatannya', 'ksihatan', 'kesihtan', 'kesiaann', 'kesiaaann', 'kesehatan']\n",
            "menghilangkannya -> ['menghilangkan', 'menghilangkn', 'menghalalkannya', 'menghidangkan', 'menghalangnya', 'kehilangannya', 'menjalankannya']\n",
            "ckaps -> ['ckap', 'caps', 'xckap', 'kapus', 'kapas', 'cskap', 'craps']\n",
            "keselmatan -> ['keselematan', 'keselamatan', 'kselamatan', 'kesempatan', 'ksempatan', 'kesesatan', 'kesemutan']\n",
            "deliverables -> ['deliveries', 'delivers', 'deliverance', 'desirable', 'deliverer', 'delivered', 'livable']\n",
            "prgss -> ['press', 'progress', 'rss', 'pss', 'prs', 'prg', 'pgs']\n",
            "neart -> ['neat', 'near', 'unearth', 'nearest', 'lineart', 'peart', 'nesar']\n",
            "unhelpful -> ['helpful', 'helpfull', 'unlawful', 'useful', 'unwell', 'unlawfully', 'ungrateful']\n",
            "kuran -> ['ukuran', 'kurban', 'kuranh', 'kurang', 'uran', 'kura', 'kuan']\n",
            "belebong -> ['lebong', 'belong', 'beleng', 'gelembong', 'celebon', 'belongs', 'abelong']\n",
            "readying -> ['reading', 'treading', 'relaying', 'readings', 'dreading', 'readin', 'threading']\n",
            "homemedic -> ['comedic', 'memei', 'memed', 'medic', 'homme', 'homem', 'homee']\n",
            "ppktapah -> ['tapah', 'papah', 'kapah', 'pktaan', 'kakpah', 'tapenah', 'tapelah']\n",
            "zaterdag -> ['tertag', 'terkdg', 'terdua', 'terdgr', 'terang', 'aterra', 'ateang']\n",
            "damat -> ['dama', 'daat', 'amat', 'diamati', 'diamant', 'adamant', 'tamat']\n",
            "misraabu -> ['miraa', 'israa', 'srabut', 'srabuk', 'saambu', 'risaau', 'misran']\n",
            "mekniscosmetics -> ['nitacosmetics', 'kameliacosmetics', 'olumiscosmetic', 'cosmetics', 'ifxcosmetics', 'anascosmetic', 'cosmetic']\n",
            "gingelly -> ['nelly', 'ingey', 'giney', 'ngebully', 'tingly', 'singel', 'pingey']\n",
            "yangvada -> ['tangada', 'yanda', 'yanaa', 'ngada', 'avada', 'angad', 'yanaaa']\n",
            "phylogeny -> ['progeny', 'halogen', 'physiology', 'phsycology', 'phony', 'phoen', 'phonenya']\n",
            "dibolak -> ['ditolak', 'dolak', 'bolak', 'diolok', 'diolah', 'dimbak', 'dielak']\n",
            "capoi -> ['capo', 'capi', 'caoi', 'apoi', 'chapo', 'chaoi', 'capui']\n",
            "wontch -> ['wtch', 'woth', 'wont', 'wronh', 'worth', 'wonho', 'witch']\n",
            "footballmanager -> ['footballer', 'footballing', 'footballeur', 'footballers', 'football', 'efootball', 'foodmonger']\n",
            "pbayaran -> ['bayaran', 'pembayaran', 'bayarkan', 'byaran', 'bayaan', 'pelayaran', 'pmbyran']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACUkQy89R0XU",
        "colab_type": "text"
      },
      "source": [
        "# Comparing similar words to other words in the text to predict unknown word\n",
        "\n",
        "- Words in the sentence are counted as context. Use these words to compare with difflib output.\n",
        "- Find the highest embeddings similarity "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEEecaPx-5cO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find the most similar word for every words in a sentence.\n",
        "# Words that doesn't exist in the vocab, will be marked as NE \n",
        "def get_all_close_word_embeddings(sentence):\n",
        "  tokens = sentence.split()\n",
        "  sequence = pd.DataFrame()\n",
        "  topn=10\n",
        "  for i in range(len(tokens)):\n",
        "    try:\n",
        "      result = model.wv.most_similar(tokens[i], topn=topn)\n",
        "      result = pd.DataFrame([w for w, s in result], columns=[tokens[i]])\n",
        "      sequence = pd.concat([sequence, result], axis=1)\n",
        "    except Exception:\n",
        "      sequence = pd.concat([sequence, pd.DataFrame(['<NE>']*topn, columns=[tokens[i]])], \n",
        "                          axis=1)\n",
        "  return pd.DataFrame(sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFBtUuAkMjAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def count_words(words):\n",
        "  d = {}\n",
        "  for w in words:\n",
        "    if w in d: d[w] += 1\n",
        "    else: d[w] = 1\n",
        "  return {k: v for k, v in sorted(d.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "def find_closest_string_with_embeddings(word, sentence):\n",
        "  tokens = sentence.split()\n",
        "  results = difflib.get_close_matches(word, model_vocab, n=7)\n",
        "  simmilarity_score, similar_word = [], []\n",
        "  for token in tokens:\n",
        "    try:\n",
        "      # get similarity score for all tokens vs difflib result\n",
        "      similarity_score = [model.wv.similarity(r, token) for r in results]\n",
        "      # find highest scoring word from each results\n",
        "      similar_word.append(results[similarity_score.index(max(similarity_score))])\n",
        "    except KeyError:\n",
        "      # Remove words to be predicted\n",
        "      # print(\"Word {} not in vocab. === Skipping this word ===\".format(token))\n",
        "      pass\n",
        "\n",
        "  print(\"Unknown Word -> {}\".format(word))\n",
        "  print(\"Full sentence -> {}\".format(sentence))\n",
        "  print(\"Similar words -> {}\".format(results))\n",
        "  print(\"Similar by embedding -> {}\".format(similar_word))\n",
        "  print(\"Most similar word -> {}\".format(count_words(similar_word)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3FHYX1dJKSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyJ6hDDJxrYt",
        "colab_type": "text"
      },
      "source": [
        "# Correct results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cycrnD_N3O6O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "d261f181-12c0-442a-d82d-3ea52fcc08b0"
      },
      "source": [
        "word_tests = ['moleq', 'lariss', 'sesssion', 'dipermudohkn', 'dinobat']\n",
        "for word in word_tests:\n",
        "  text = get_sentence(word)\n",
        "  find_closest_string_with_embeddings(word, text)\n",
        "  print()"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unknown Word -> moleq\n",
            "Full sentence -> gym pom dh sepi last day before officially lock down yg mane diberi cuti tu sila duk diam kt rumah yg mane kerja tu selamat bekerja jaga diri moleq blako jangan kerana corona hilang iman pedoman\n",
            "Similar words -> ['mole', 'moole', 'moles', 'molep', 'molek', 'moleh', 'mohle']\n",
            "Similar by embedding -> ['mole', 'molek', 'molek', 'moleh', 'mole', 'moles', 'mole', 'mohle', 'mole', 'mole', 'molek', 'molek', 'moleh', 'molek', 'molek', 'molek', 'molek', 'molek', 'molek', 'molek', 'molek', 'molek', 'molek', 'molek', 'moole', 'moleh', 'molek', 'molek', 'molep', 'molek', 'moleh', 'mole', 'molek', 'molek', 'mole']\n",
            "Most similar word -> {'molek': 20, 'mole': 7, 'moleh': 4, 'moles': 1, 'mohle': 1, 'moole': 1, 'molep': 1}\n",
            "\n",
            "Unknown Word -> lariss\n",
            "Full sentence -> harap ketupat paling lariss y\n",
            "Similar words -> ['larissa', 'larss', 'laris', 'laiss', 'clarissa', 'mariss', 'laisse']\n",
            "Similar by embedding -> ['laris', 'laris', 'laris', 'laiss']\n",
            "Most similar word -> {'laris': 3, 'laiss': 1}\n",
            "\n",
            "Unknown Word -> sesssion\n",
            "Full sentence -> covid pendamic chow kit wet market disinfection sesssion a potrait of city hall worker during the disinfection process at the local wet market in kuala lumpur malaysia\n",
            "Similar words -> ['session', 'sessions', 'posession', 'obsession', 'cession', 'succession', 'sessionist']\n",
            "Similar by embedding -> ['succession', 'posession', 'sessionist', 'obsession', 'obsession', 'sessions', 'sessions', 'sessions', 'session', 'sessions', 'sessions', 'session', 'obsession', 'sessions', 'sessions', 'sessions', 'succession', 'sessions', 'sessions', 'obsession', 'obsession', 'sessions', 'sessions', 'sessions', 'sessions', 'succession']\n",
            "Most similar word -> {'sessions': 14, 'obsession': 5, 'succession': 3, 'session': 2, 'posession': 1, 'sessionist': 1}\n",
            "\n",
            "Unknown Word -> dipermudohkn\n",
            "Full sentence -> semoga dipermudohkn urusan pembahagian pada penduduk tdtd dapat meringankan sedikit beban bagi yang kurang berkemampuan\n",
            "Similar words -> ['dipermudhkn', 'dipermudhkan', 'dipermudahkn', 'dpermudhkn', 'diprmudhkn', 'dipermdhkn', 'dipermudahkan']\n",
            "Similar by embedding -> ['dipermudahkan', 'dipermudahkan', 'dipermdhkn', 'dipermudahkan', 'dipermudahkan', 'dipermudahkan', 'dipermudhkn', 'dipermudhkn', 'dipermudahkan', 'dipermudahkan', 'dipermudahkan', 'dipermudahkan', 'dipermudahkan']\n",
            "Most similar word -> {'dipermudahkan': 10, 'dipermudhkn': 2, 'dipermdhkn': 1}\n",
            "\n",
            "Unknown Word -> dinobat\n",
            "Full sentence -> terpegun dengar jawapan datuk dr ketua pengarah kesihatan ketika jawab soalan sukar media boleh dapat pelbagai ilmu baru patutlah dinobat antara terbaik kendali kes di dunia\n",
            "Similar words -> ['diobati', 'dingbat', 'nobat', 'inoba', 'dinot', 'dinobatkan', 'dikoba']\n",
            "Similar by embedding -> ['inoba', 'nobat', 'nobat', 'dinobatkan', 'nobat', 'dinobatkan', 'dinobatkan', 'nobat', 'dinobatkan', 'nobat', 'dinobatkan', 'dinobatkan', 'nobat', 'nobat', 'dinobatkan', 'dinobatkan', 'dinobatkan', 'nobat', 'nobat', 'dinobatkan', 'dinobatkan', 'dinot', 'dinobatkan', 'dinobatkan', 'dinobatkan']\n",
            "Most similar word -> {'dinobatkan': 14, 'nobat': 9, 'inoba': 1, 'dinot': 1}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHfxMlKV6F3c",
        "colab_type": "text"
      },
      "source": [
        "1) For *'moleq'*, *'lariss'* and *'session'*, the most similar word produced by difflib is wrong, but the correct word is still in the list. By comparing word embeddings, the correct word is recognized.\n",
        "\n",
        "2) For *'sesssion'*, *'dipermudohkan'* and *'dinobat'*, difflib works well to find all simmilar words as almost all the words, including the most similar are correct. When combine with embedding, higher context word is chosen. This is caused by frequency of the word in the training corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6Bd3QShX8Dx",
        "colab_type": "text"
      },
      "source": [
        "# Wrong Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGHqxxyZ4_YJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "5d62bd44-95da-44bc-be64-174dc3a1698c"
      },
      "source": [
        "word_tests = ['reassemble', 'compasses', 'boleeeeeh', 'meningatkan', 'bomathi']\n",
        "for word in word_tests:\n",
        "  text = get_sentence(word)\n",
        "  find_closest_string_with_embeddings(word, text)\n",
        "  print()"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unknown Word -> reassemble\n",
            "Full sentence -> day avengers reassemble psst admin kka sorg tu blh start charging suit dah mrs potts stark kan from malaysiaigers level pkp fasa polis fasa tentera fasa\n",
            "Similar words -> ['ressemble', 'ressemblez', 'ressembles', 'ressembler', 'resemble', 'assemble', 'ressemblent']\n",
            "Similar by embedding -> ['resemble', 'ressemblent', 'ressemblez', 'ressemblent', 'ressemble', 'ressembles', 'ressemblent', 'ressembles', 'resemble', 'assemble', 'ressembles', 'ressemblent', 'ressembles', 'ressembler', 'ressembles', 'assemble', 'assemble', 'assemble', 'assemble', 'ressembler', 'assemble', 'ressembler', 'assemble']\n",
            "Most similar word -> {'assemble': 7, 'ressembles': 5, 'ressemblent': 4, 'ressembler': 3, 'resemble': 2, 'ressemblez': 1, 'ressemble': 1}\n",
            "\n",
            "Unknown Word -> compasses\n",
            "Full sentence -> thank you and all workers for being there for us in all over the world it s time to reset our compasses ramp up compassion kindness to everyone we can beat this\n",
            "Similar words -> ['encompasses', 'compass', 'compares', 'passes', 'masses', 'encompassed', 'compas']\n",
            "Similar by embedding -> ['compares', 'masses', 'masses', 'masses', 'masses', 'compass', 'masses', 'masses', 'compass', 'masses', 'compass', 'masses', 'masses', 'masses', 'masses', 'masses', 'passes', 'masses', 'masses', 'compass', 'masses', 'passes', 'compass', 'masses', 'masses', 'masses', 'masses', 'masses', 'passes', 'compares', 'masses']\n",
            "Most similar word -> {'masses': 21, 'compass': 5, 'passes': 3, 'compares': 2}\n",
            "\n",
            "Unknown Word -> boleeeeeh\n",
            "Full sentence -> cooking a sikit for our volunteer lunch tadi diet apa diet kita diet esok pun boleeeeeh\n",
            "Similar words -> ['boleeeeh', 'boleeeee', 'boleeeeee', 'boleeeh', 'boleeee', 'leeeee', 'eeeeeh']\n",
            "Similar by embedding -> ['boleeeeh', 'eeeeeh', 'boleeeh', 'eeeeeh', 'boleeeh', 'boleeee', 'eeeeeh', 'boleeeh', 'eeeeeh', 'boleeee', 'eeeeeh', 'leeeee', 'eeeeeh', 'eeeeeh', 'boleeeee']\n",
            "Most similar word -> {'eeeeeh': 7, 'boleeeh': 3, 'boleeee': 2, 'boleeeeh': 1, 'leeeee': 1, 'boleeeee': 1}\n",
            "\n",
            "Unknown Word -> meningatkan\n",
            "Full sentence -> saudara saudari rakyat dll mari kita saling bantu meningatkan satu sama lain perjuangan untuk menghentikan penularan boleh di menang sayangi pekerja dll\n",
            "Similar words -> ['meningkatkan', 'mengingatkan', 'mngingatkan', 'meningalkan', 'mengingatkn', 'mengingatkanku', 'meninggalkan']\n",
            "Similar by embedding -> ['mengingatkan', 'mengingatkan', 'meningkatkan', 'meningkatkan', 'mengingatkn', 'mengingatkan', 'mengingatkan', 'meningkatkan', 'meningkatkan', 'meninggalkan', 'mengingatkan', 'mengingatkan', 'meningkatkan', 'meningkatkan', 'meningkatkan', 'meningkatkan', 'meninggalkan', 'mengingatkan', 'mengingatkan', 'meningkatkan', 'meningkatkan']\n",
            "Most similar word -> {'meningkatkan': 10, 'mengingatkan': 8, 'meninggalkan': 2, 'mengingatkn': 1}\n",
            "\n",
            "Unknown Word -> bomathi\n",
            "Full sentence -> hadive udhares nufenevi dhane ey bandilaa nagaa bomathi koh nimijjey varugadha kolhigandeh ge vai rajje ah misraabu vejje anna oiy thoofaanun salamaiy vumah zuvaabu nukoh kudhi bodu enmen masahkaiy kuran vejje ge balimadukamun alhamen minju kuravaafaandeyve aameen\n",
            "Similar words -> ['mathi', 'bathi', 'basmathi', 'momati', 'mathri', 'mathis', 'bumati']\n",
            "Similar by embedding -> ['mathri', 'bumati', 'bumati', 'bathi', 'bumati', 'mathri', 'mathri', 'bumati', 'mathri', 'bathi', 'bumati', 'mathri']\n",
            "Most similar word -> {'mathri': 5, 'bumati': 5, 'bathi': 2}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b49ISA3w8yD5",
        "colab_type": "text"
      },
      "source": [
        "1) For *'reassemble'*, *'compasses'*, *'boleeeeeh'* and *'meningatkan'*, difflib does a good job to find the most similar word. Most of the embedding score also produce the correct words. However, since this experiment uses every word in the sentence, more noise are included in the similar by embedding list, outnumbering the correct word. This can be observed from Most similar word list, where the second word for *'compasses'*, *'boleeeeeh'*, *'meningatkan'* is the correct word.\n",
        "\n",
        "2) *'bomathi'* is not a malay or english word (or it is?). When plotting every word in the sentence, the result shows that most of the words are not in the vocab. While it still gives output, I have no idea if that is correct."
      ]
    }
  ]
}